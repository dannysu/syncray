#!/usr/bin/env python

import os
import shutil
import sys
import subprocess
import json
import hashlib
import argparse
import re
import getpass
import tempfile
import time
from logging import error

prog = 'syncray'
tmp_dir = tempfile.mkdtemp()

class AmazonS3:
    def download(self, remote, local):
        proc = subprocess.Popen(['s3cmd', 'get', '--force', remote, local], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return proc.communicate()

    def upload(self, local, remote):
        proc = subprocess.Popen(['s3cmd', 'put', local, remote], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return proc.communicate()

    def delete(self, remote):
        proc = subprocess.Popen(['s3cmd', 'del', remote], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return proc.communicate()

    def md5(self, remote):
        proc = subprocess.Popen(['s3cmd', 'info', remote], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, errors = proc.communicate()
        if proc.returncode != 0:
            raise RuntimeError("Failed to get info for " + remote)
        
        m = re.search('MD5 sum: *([a-z0-9]*)', output)
        return m.group(1)

class JoyentManta:
    # Manta can't handle spaces in filename, so need this conversion function.
    def convert(self, path):
        parts = split_path(path)

        fixed_parts = []
        for part in parts:
            if part.find(" ") > 0:
                m = hashlib.md5()
                m.update(part)
                md5hash = m.hexdigest()
                part = md5hash
            fixed_parts.append(part)

        path = os.sep + os.sep.join(fixed_parts)
        return path

    def download(self, remote, local, retries=0):
        remote = self.convert(remote)
        proc = subprocess.Popen(['mget', '-o', local, remote], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, errors = proc.communicate()
        if proc.returncode != 0:
            if retries < 2:
                time.sleep(.5)
                return self.download(remote, local, retries + 1)
            else:
                return output, errors
        else:
            return output, errors

    def upload(self, local, remote, retries=0):
        dirname = os.path.dirname(remote)

        dirname = self.convert(dirname)
        proc = subprocess.Popen(['mmkdir', '-p', dirname], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, errors = proc.communicate()
        if proc.returncode != 0:
            if retries < 2:
                time.sleep(.5)
                return self.upload(local, remote, retries + 1)
            else:
                raise RuntimeError("Failed to mmkdir -p " + dirname)

        remote = self.convert(remote)
        proc = subprocess.Popen(['mput', '-c', '1', '-f', local, remote], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return proc.communicate()

    def delete(self, remote, retries=0):
        remote = self.convert(remote)
        proc = subprocess.Popen(['mrm', remote], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, errors = proc.communicate()
        if proc.returncode != 0:
            if retries < 2:
                time.sleep(.5)
                return self.delete(remote, retries + 1)
            else:
                return output, errors
        else:
            return output, errors

    def md5(self, remote):
        remote = self.convert(remote)
        proc = subprocess.Popen(['mmd5', remote], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, errors = proc.communicate()
        if proc.returncode != 0:
            raise RuntimeError("Failed to get info for " + remote)
        
        m = re.search('^([a-z0-9]*) ', output)
        return m.group(1)

def encrypt(decrypted, encrypted, passphrase):
    data = {"output": encrypted, "input": decrypted}
    gpg_encrypt = 'gpg -c --cipher-algo AES256 --no-verbose --no-use-agent --batch --yes --passphrase-fd 0 -o %(output)s %(input)s'
    parts = gpg_encrypt.split(" ")
    command = []
    for part in parts:
        command.append(part % data)
    proc = subprocess.Popen(command, stdin = subprocess.PIPE, stdout = subprocess.PIPE, stderr = subprocess.PIPE)
    output, errors = proc.communicate(passphrase + "\n")
    if proc.returncode != 0:
        print errors
        raise RuntimeError("Failed to encrypt " + decrypted)

def decrypt(encrypted, decrypted, passphrase):
    data = {"output": decrypted, "input": encrypted}
    gpg_decrypt = 'gpg -d --cipher-algo AES256 --no-verbose --no-use-agent --batch --yes --passphrase-fd 0 -o %(output)s %(input)s'
    parts = gpg_decrypt.split(" ")
    command = []
    for part in parts:
        command.append(part % data)
    proc = subprocess.Popen(command, stdin = subprocess.PIPE, stdout = subprocess.PIPE, stderr = subprocess.PIPE)
    output, errors = proc.communicate(passphrase + "\n")
    if proc.returncode != 0:
        print errors
        raise RuntimeError("Failed to decrypt " + encrypted)

# Calculates the MD5 of a file
def md5(path):
    m = hashlib.md5()

    with open(path,'rb') as f:
        while True:
            chunk = f.read(8192)
            if not chunk: break
            m.update(chunk)

    return m.hexdigest()

def move(src, dst):
    dirname = os.path.dirname(dst)
    try:
        os.makedirs(dirname)
    except OSError:
        pass
    shutil.move(src, dst)

def list(directory):
    result = []
    for (path, dirs, files) in os.walk(directory):
        for filename in files:
            result.append(path + os.sep + filename)
    return result

def split_path(path):
    if path.endswith(os.sep):
        path = path[:len(path)-1]
    parts = []
    while True:
        dirname, basename = os.path.split(path)

        if basename != "":
            parts.append(basename)
            path = dirname
        else:
            break

    parts.reverse()
    return parts

def get_remote_path(local_path):
    parts = split_path(local_path)
    fixed_parts = []
    for part in parts:
        regex = re.compile("[^\x00-\x7F]+")
        findall = regex.findall(part)
        if len(findall) > 0:
            m = hashlib.md5()
            m.update(part)
            md5hash = m.hexdigest()
            part = md5hash
        fixed_parts.append(part)

    remote_path = os.sep.join(fixed_parts)
    return remote_path

def download_metadata(bucket, directory, service, passphrase):
    remote = bucket + '/.syncray'
    tmp_encrypt = tmp_dir + os.sep + directory + '.syncray.encrypt'
    tmp_decrypt = tmp_dir + os.sep + directory + '.syncray.decrypt'
    local = tmp_dir + os.sep + directory + '.syncray'

    output, errors = service.download(remote, tmp_encrypt)

    if "404 (Not Found)" in errors or "ResourceNotFoundError" in errors:
        # No metadata found for the directory to be backed up
        metadata = {}
    else:
        decrypt(tmp_encrypt, tmp_decrypt, passphrase)
        move(tmp_decrypt, local)

        f = open(local)
        metadata = json.loads(f.read())
        f.close()

    os.remove(tmp_encrypt)

    return metadata

def upload_metadata(bucket, directory, metadata, service, passphrase):
    remote = bucket + '/.syncray'
    tmp_encrypt = tmp_dir + os.sep + directory + '.syncray.encrypt'
    local = tmp_dir + os.sep + directory + '.syncray'

    # Save updated metadata file to disk
    output = json.dumps(metadata)
    f = open(local, "w")
    f.write(output)
    f.close()

    # Encrypt it
    encrypt(local, tmp_encrypt, passphrase)

    output, errors = service.upload(tmp_encrypt, remote)
    if errors != "":
        raise RuntimeError("Failed to update metadata")

    os.remove(tmp_encrypt)

def download_file(bucket, directory, entry, service, passphrase):
    if len(entry['files']) == 1:
        file_entry = entry['files'][0]
        remote = bucket + '/' + file_entry['remote_path']
        tmp_encrypt = tmp_dir + os.sep + os.path.basename(entry['local_path']) + '.encrypt'
        tmp_decrypt = tmp_dir + os.sep + os.path.basename(entry['local_path']) + '.decrypt'
        local = directory + os.sep + entry['local_path']

        output, errors = service.download(remote, tmp_encrypt)
        if errors != "":
            raise RuntimeError("Failed to download file " + local)

        decrypt(tmp_encrypt, tmp_decrypt, passphrase)
        move(tmp_decrypt, local)

        os.remove(tmp_encrypt)

        return

    cat_cmd = ['cat']

    entries = entry['files']
    entries.sort(key=lambda e: e['local_path'])
    for file_entry in entries:
        remote = bucket + '/' + file_entry['remote_path']
        tmp_encrypt = tmp_dir + os.sep + os.path.basename(file_entry['local_path']) + '.encrypt'
        tmp_decrypt = tmp_dir + os.sep + os.path.basename(file_entry['local_path']) + '.decrypt'
        local = tmp_dir + os.sep + os.path.basename(file_entry['local_path'])

        output, errors = service.download(remote, tmp_encrypt)
        if errors != "":
            raise RuntimeError("Failed to download file " + local)
        
        decrypt(tmp_encrypt, tmp_decrypt, passphrase)
        move(tmp_decrypt, local)

        cat_cmd.append(local)

        os.remove(tmp_encrypt)

    local = open(directory + os.sep + entry['local_path'], 'w')
    proc = subprocess.Popen(cat_cmd, stdout=local, stderr=subprocess.PIPE)
    output, errors = proc.communicate()
    if errors != "":
        raise RuntimeError("Failed to concatenate files for " + directory + os.sep + entry['path'])

    local.close()

def upload_file(bucket, directory, path, service, passphrase):
    statinfo = os.stat(path)

    pending_files = []

    # Split large files into smaller chunks
    chunk_size = 100
    if statinfo.st_size >= chunk_size * 1024 * 1024:
        dirname = os.path.dirname(path)
        filename = os.path.basename(path)
        proc = subprocess.Popen(['split', '-b' + str(chunk_size) + 'm', path, tmp_dir + os.sep + filename + '-'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        output, errors = proc.communicate()
        if errors != "":
            raise RuntimeError("Failed to split file " + path)

        for (sub_path, dirs, files) in os.walk(tmp_dir):
            for file in files:
                if file.startswith(filename + '-'):
                    md5hash = md5(tmp_dir + os.sep + file)
                    local_path = dirname[len(directory)+1:] + os.sep + file
                    remote_path = get_remote_path(local_path)
                    pending_files.append({
                        "local_path": local_path,
                        "remote_path": remote_path,
                        "orig_md5": md5hash,
                        "tmp_path": tmp_dir + os.sep + file
                    })
    else:
        local_path = path[len(directory)+1:]
        remote_path = get_remote_path(local_path)
        pending_files.append({
            "local_path": local_path,
            "remote_path": remote_path,
            "orig_md5": md5(path),
            "tmp_path": path
        })

    for file_entry in pending_files:
        tmp_path = file_entry['tmp_path']
        tmp_encrypt = tmp_dir + os.sep + 'tmp.encrypt'
        file_path = file_entry['remote_path']

        encrypt(tmp_path, tmp_encrypt, passphrase)

        if file_path.startswith(os.sep):
            remote = bucket + file_path
        else:
            remote = bucket + os.sep + file_path

        output, errors = service.upload(tmp_encrypt, remote)
        if errors != "":
            print errors
            raise RuntimeError("Failed to upload file " + tmp_path)

        file_entry['encrypted_md5'] = md5(tmp_encrypt)

        os.remove(tmp_encrypt)

        file_entry.pop('tmp_path')

    return pending_files

def backup(bucket, rel_dir, mode, verbose, verify, passphrase, service):
    directory = os.path.abspath(rel_dir)
    basename = os.path.basename(directory)

    if verbose:
        print 'Backing up ' + directory + ' to ' + bucket

    # Obtain metadata containing sync state from S3
    if verbose:
        print 'Download metadata'
    metadata = download_metadata(bucket, basename, service, passphrase)

    # Go through each file in directory to find what changed
    if verbose:
        print 'Find changes'
    pending = []
    files = list(directory)
    for path in files:
        key = path[len(directory)+1:]
        key = get_remote_path(key)
        if key in metadata:
            # file is in metadata, so check to see if it has changed
            md5hash = md5(path)
            if metadata[key]["orig_md5"] != md5hash:
                pending.append({"path": path, "orig_md5": md5hash})
            elif verify:
                for part in metadata[key]['files']:
                    expected_md5 = part['encrypted_md5']
                    actual_md5 = service.md5(bucket + '/' + part['remote_path'])
                    if expected_md5 != actual_md5:
                        pending.append({"path": path, "orig_md5": md5hash})
                        break
        else:
            # file is not in metadata, so that means file is not in remote
            pending.append({"path": path, "orig_md5": md5(path)})

    # Go through each pending file, encrypt and upload them to S3, then update metadata
    if verbose:
        print 'Upload changes'
    count = 0
    total = len(pending)
    for change in pending:
        count += 1
        if verbose:
            sys.stdout.write(str(count) + " of " + str(total))
            sys.stdout.flush()

        path = change['path']
        orig_md5 = change['orig_md5']

        encrypted_files = upload_file(bucket, directory, path, service, passphrase)

        local_path = path[len(directory)+1:]
        key = local_path
        key = get_remote_path(key)
        metadata[key] = {"local_path": local_path, "orig_md5": orig_md5, "files": encrypted_files}
        upload_metadata(bucket, basename, metadata, service, passphrase)

        if verbose:
            sys.stdout.write(" " + unichr(10003) + "\n")

    # If we're in Echo mode, then also delete on remote anything already gone on local
    if mode == 'e':
        remove = []
        for key in metadata:
            entry = metadata[key]
            if not os.path.exists(directory + os.sep + entry['local_path']):
                remove.append(key)

        for key in remove:
            service.delete(bucket + '/' + key)
            metadata.pop(key)
            upload_metadata(bucket, basename, metadata, service, passphrase)

    if verbose:
        print 'Done!'

def restore(bucket, rel_dir, verbose, passphrase, service):
    directory = os.path.abspath(rel_dir)
    basename = os.path.basename(directory)

    if verbose:
        print 'Restoring ' + bucket + ' to ' + directory

    # Obtain metadata containing sync state from S3
    if verbose:
        print 'Download metadata'
    metadata = download_metadata(bucket, basename, service, passphrase)

    if verbose:
        print 'Download files'
    count = 0
    total = len(metadata.keys())
    for key in metadata:
        count += 1
        if verbose:
            sys.stdout.write(str(count) + " of " + str(total) + " ")
            sys.stdout.flush()

        entry = metadata[key]
        path = directory + os.sep + entry['local_path']
        if os.path.exists(path):
            md5hash = md5(path)
            if md5hash != entry['orig_md5']:
                download_file(bucket, directory, entry, service, passphrase)
        else:
            download_file(bucket, directory, entry, service, passphrase)

        if verbose:
            sys.stdout.write(unichr(10003) + "\n")

    if verbose:
        print 'Done!'

def main():
    description = prog + """ is a tool for synchronizing folders and encrypt
                         them to cloud storage encrypted such as Amazon S3 and
                         Joyent Manta."""
    parser = argparse.ArgumentParser(description = description,
            prog = prog)

    parser.add_argument('-s', nargs='?', default='s3',
            metavar='service',
            choices=['s3', 'manta'],
            dest='service',
            help='cloud storage to use {%(choices)s} (default: %(default)s)')

    parser.add_argument('-m', nargs='?', default='contribute',
            metavar='mode',
            choices=['contribute', 'echo'],
            dest='mode',
            help="""sync mode to use {%(choices)s}. 'contribute' copies only
                 from local to remote, no deletes. 'echo' replicates changed
                 files and deletes to remote. (default: %(default)s)""")

    parser.add_argument('--verbose', dest='verbose', action='store_true')
    parser.add_argument('--verify', dest='verify', action='store_true',
            help="""verify remote files' md5 against metadata and resync if
                 there is a mismatch""")

    parser.add_argument('-p', nargs='?', metavar='passphrase', dest='passphrase',
            help="set passphrase for encryption so you don't have to type it in")

    parser.add_argument('action', nargs=1,
            choices=['backup', 'restore'],
            help='backup or restore data')

    remote_help = """s3 bucket (s3://bucket-name/folder) or Manta path
                  (/$MANTA_USER/stor/folder)"""
    parser.add_argument('remote', nargs=1, help=remote_help)

    parser.add_argument('local', nargs=1,
            help="directory on local computer")

    args = parser.parse_args()

    action = args.action[0]
    remote = args.remote[0]
    local = args.local[0]
    service = args.service[0]
    mode = args.mode[0]
    passphrase = args.passphrase
    verbose = args.verbose
    verify = args.verify

    if passphrase == None:
        passphrase = getpass.getpass("Enter passphrase to encrypt your data with: ")

    if service == 's':
        service = AmazonS3()
    else:
        service = JoyentManta()

    if action == 'backup':
        backup(remote, local, mode, verbose, verify, passphrase, service)
    elif action == "restore":
        restore(remote, local, verbose, passphrase, service)

    shutil.rmtree(tmp_dir)

if __name__ == '__main__':
    try:
        main()
        sys.exit(0)

    except SystemExit, e:
        sys.exit(e.code)

    except KeyboardInterrupt:
        sys.exit(1)

    except Exception, e:
        error(u"Unknown problem: %s" % e)
        sys.exit(1)
